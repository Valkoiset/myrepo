{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning in Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Flux] (http://fluxml.ai/) is a Julia library designed to create machine learning models.\n",
    "- It is written entirely in Julia, which makes it trivial to modify it and adapt it to your needs.\n",
    "- It is possible to use inside of it Julia syntax, functions and macros.\n",
    "- Creating complex models is intuitive and fast, it usually takes only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we are going to design a MLP for classifying handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, Tracker, @epochs\n",
    "using Base.Iterators: repeated\n",
    "using Distributed\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify MNIST digits with a simple multi-layer-perceptron\n",
    "\n",
    "@everywhere const TRAIN = (class=UInt8[], image=Matrix{Int32}[])\n",
    "@everywhere const TEST = (class=UInt8[], image=Matrix{Int32}[])\n",
    "\n",
    "@everywhere for (filename, data) in [(\"mnist_train.int\", TRAIN),\n",
    "                                     (\"mnist_test.int\", TEST)]\n",
    "    open(filename) do f\n",
    "        while !eof(f)\n",
    "            c = read(f, UInt8)\n",
    "            v = read(f, 28^2)\n",
    "            push!(data.class, c)\n",
    "            push!(data.image, reshape(v, 28, 28))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "imshow(TRAIN.image[1])\n",
    "println(TRAIN.class[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stack images into one large batch\n",
    "X = hcat(float.(reshape.(TRAIN.image, :))...) |> gpu;\n",
    "# One-hot-encode the labels\n",
    "Y = onehotbatch(TRAIN.class, 0:9) |> gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stack images into one large batch\n",
    "tX = hcat(float.(reshape.(TEST.image, :))...) |> gpu;\n",
    "# One-hot-encode the labels\n",
    "tY = onehotbatch(TEST.class, 0:9) |> gpu;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is ready we should start designing the DL model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with manually designing a perceptron with sigmoidal activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rand(4, 8)\n",
    "b = rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₁(x) = 1.0 ./ (1.0.+exp.(-W*x - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(8)\n",
    "layer₁(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model in Flux, we need to inform Flux, that those parameters: <tt>W</tt> and <tt>b</tt> should be trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker\n",
    "\n",
    "W = param(W)\n",
    "b = param(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually all of the above is already defined in Flux, where you can find most popular [activation functions](http://fluxml.ai/Flux.jl/stable/models/layers.html#Activation-Functions-1), which we can use in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₂(x) = σ.(W * x .+ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₂(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies to defining [model layers](http://fluxml.ai/Flux.jl/stable/models/layers.html#Basic-Layers-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₃ = Dense(8,4,σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer₃(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we cannot find a suitable definition in Flux, we can declare our own layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Affine\n",
    "  W\n",
    "  b\n",
    "end\n",
    "\n",
    "Affine(in::Integer, out::Integer) =\n",
    "  Affine(param(randn(out, in)), param(randn(out)))\n",
    "\n",
    "# Overload call, so the object can be used as a function\n",
    "(m::Affine)(x) = m.W * x .+ m.b\n",
    "\n",
    "a = Affine(10, 5)\n",
    "\n",
    "a(rand(10)) # => 5-element vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully use Flux functionalities, the following line with calling macro <tt>treelike</tt> must be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Flux.@treelike Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model can consist of more layers than one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer₁ = Dense(28^2, 32, relu)\n",
    "Layer₂ = Dense(32, 10)\n",
    "Layer₃ = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₁ = Chain(Layer₁ , Layer₂, Layer₃) |> gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>Chain</tt> function allows to join arbitrary functions into execution chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chain = Chain(x -> x^2, x-> -x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₂(x) = Layer₃(Layer₂(Layer₁(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as a function composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₃(x) = Layer₁ ∘ Layer₂ ∘ Layer₃  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m₄(x) = Layer₁(x) |> Layer₂  |> Layer₃ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the model defined we can move on with defining the loss function for the model and its regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function, regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can declare the loss function manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Dense(5,2)\n",
    "\n",
    "x, y = rand(5), rand(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(ŷ, y) = sum((ŷ.- y).^2)/ length(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(model(x), y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [one of the already defined in Flux:](https://github.com/FluxML/Flux.jl/blob/8f73dc6e148eedd11463571a0a8215fd87e7e05b/src/layers/stateless.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.mse(model(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also [regularization](http://fluxml.ai/Flux.jl/stable/models/regularisation.html) is [intuitive](http://fluxml.ai/Flux.jl/stable/models/layers.html#Normalisation-and-Regularisation-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty() =  norm(model.W) + norm(model.b)\n",
    "loss(ŷ,y) = Flux.mse(ŷ,y) + penalty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(model(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(ŷ,y) = Flux.mse(ŷ,y) + sum(norm,params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(model(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other regularization techniques can be implemented [as layers:](http://fluxml.ai/Flux.jl/stable/models/layers.html#Normalisation-and-Regularisation-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Chain(Dense(28^2, 32, relu),\n",
    "    Dropout(0.1),\n",
    "Dense(32, 10),\n",
    "BatchNorm(64, relu),\n",
    "softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the designed model loss function can look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss(x, y) = crossentropy(m₁(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model we can move on with training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main element that allows a proper training proces is an appropriate algorithm for computing gradients. In Flux it looks as presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "# df/dx = 6x + 2\n",
    "df(x) = Tracker.gradient(f, x)[1]\n",
    "\n",
    "df(2) # 14.0 (tracked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When function has many variables weights can be kept as a collection and passed to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = param(2) # 2.0 (tracked)\n",
    "b = param(3) # 3.0 (tracked)\n",
    "\n",
    "f(x) = W * x + b\n",
    "\n",
    "par = Flux.Params([W, b])\n",
    "grads = Tracker.gradient(() -> f(4), par)\n",
    "\n",
    "grads[W] # 4.0\n",
    "grads[b] # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux can control the whole training process, we don't need to implement that manually. A special function <tt>train!</tt> is designed for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Flux.train!(objective, data, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback is that it allows to run training only for one epoch. To iteratively train the model over the available dataset we either replicate the dataset appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = repeated((X, Y), 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use the <tt>@epochs</tt> macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Flux.@epochs 2 println(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It allows us to define functions, that are called during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalcb = () -> @show(loss(tX, tY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on with gathering it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify MNIST digits with a simple multi-layer-perceptron\n",
    "\n",
    "# Stack images into one large batch\n",
    "X = hcat(float.(reshape.(TRAIN.image, :))...) |> gpu;\n",
    "# One-hot-encode the labels\n",
    "Y = onehotbatch(TRAIN.class, 0:9) |> gpu;\n",
    "\n",
    "# Stack images into one large batch\n",
    "tX = hcat(float.(reshape.(TEST.image, :))...) |> gpu;\n",
    "# One-hot-encode the labels\n",
    "tY = onehotbatch(TEST.class, 0:9) |> gpu;\n",
    "\n",
    "m = Chain(\n",
    "  Dense(28^2, 320, relu),\n",
    "  Dense(320, 10),\n",
    "  softmax) |> gpu\n",
    "                                                                                                                                                                                                                                                                                                                            \n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "                                                                                                                                                    \n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))\n",
    "\n",
    "dataset = (X, Y)\n",
    "evalcb = () -> @show(accuracy(tX, tY))\n",
    "opt = ADAM(params(m))\n",
    "\n",
    "@epochs 200 Flux.train!(loss, [dataset], opt, cb = throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without GPU this training process can be quite long. Julia natively supports moving the computation to  the [GPU](http://fluxml.ai/Flux.jl/stable/gpu.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux is not the only machine/deep learning framework in Julia. Below are listed other, that can also be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Knet.jl](https://github.com/denizyuret/Knet.jl)\n",
    "- [MXnet.jl](https://github.com/dmlc/MXNet.jl)\n",
    "- [TensorFlow.jl](https://github.com/malmaud/TensorFlow.jl)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "E78C04D8EF544306A8BDAB16BBC12BE0",
   "lastKernelId": "27d72737-4274-48c8-ac47-b8b4a7d86789"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
