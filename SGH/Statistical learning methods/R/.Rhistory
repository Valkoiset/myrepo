table(random.labels)
split.data<-split(Hitters,random.labels)
train <- split.data$train
validate <-split.data$validate
test <- split.data$test
# 4) [2p] Identify variables with correlation at least at 0.5 using correlation test (to determine the
# correlation and its significance use instruction: ?cor.test?).
cor.test(Hitters$Salary, Hitters$AtBat) # Y
# 2) Load dataset named ?Hitters? {data(Hitters)}.
data("Hitters")
# 3) Set the seed according to the number of your index.
set.seed(83459)
# To get final results go through mini tasks:
library(mgcv)
library(ROCR)
library(Ecdat)
# dev.off()
hist(Hitters$Salary)
typeof(Hitters$Salary)
shapiro.test(Hitters$Salary)
?shapiro.test
any(is.na(Hitters))
summary(Hitters)
# Kt?re zmienne maj? braki
which(apply(is.na(Hitters), 2, any)==T)
# Hitters$Salary <- as.numeric(data$Salary)
Hitters$Salary[is.na(Hitters$Salary)] <- median(Hitters$Salary, na.rm=T)
any(is.na(Hitters))
# Deleting columns
drops <- c("League", "Division", "NewLeague")
Hitters <- Hitters[ , !(names(Hitters) %in% drops)]
# 3) [2p] Divide equally dataset into training, validation and testing subset.
labels<-factor(rep(c("train","validate", "test"),length=nrow(Hitters)))
random.labels<-sample(labels)
head(random.labels)
table(random.labels)
split.data<-split(Hitters,random.labels)
train <- split.data$train
validate <-split.data$validate
test <- split.data$test
# 4) [2p] Identify variables with correlation at least at 0.5 using correlation test (to determine the
# correlation and its significance use instruction: ?cor.test?).
cor.test(Hitters$Salary, Hitters$AtBat) # Y
cor.test(Hitters$Salary, Hitters$Hits) # Y
cor.test(Hitters$Salary, Hitters$HmRun) # Y
cor.test(Hitters$Salary, Hitters$Runs) # Y
cor.test(Hitters$Salary, Hitters$RBI) # Y
cor.test(Hitters$Salary, Hitters$Walks) # Y
cor.test(Hitters$Salary, Hitters$Years) # Y
cor.test(Hitters$Salary, Hitters$CAtBat) # Y
cor.test(Hitters$Salary, Hitters$CHits) # Y
cor.test(Hitters$Salary, Hitters$CHmRun) # Y
cor.test(Hitters$Salary, Hitters$CRuns) # N
cor.test(Hitters$Salary, Hitters$CRBI) # Y
cor.test(Hitters$Salary, Hitters$Walks) # Y
cor.test(Hitters$Salary, Hitters$PutOuts) # Y
cor.test(Hitters$Salary, Hitters$Assists) # Y
cor.test(Hitters$Salary, Hitters$Errors) # Y
?cor.test
#Z tego drugiego pliku
cor(Hitters)
y<-c()
for (i in 1:ncol(Hitters)){
y[i]<-ifelse(cor.test(Hitters$Salary,Hitters[,i])$p.value<0.05 & as.numeric(cor.test(Hitters$Salary,Hitters[,i])[[4]])!=1,1,0)
}
for (i in 1:ncol(Hitters)){
print(cor.test(Hitters$Salary,Hitters[,i]))
}
View(train)
gam.model<-glm(Salary ~ Hitters[-c("CRuns")] ,data=train)
gam.model<-glm(Salary ~ -c("CRuns") ,data=train)
#5) [2p] Build regression with GLM on training subset with variables, which have correlation over
# 0,5. Which variables are statistically significant?
glm.model<-glm(Salary ~ CRuns, Hits, HmRun, Runs, Walks, Years, CAtBat, CHits, CHmRun,
CRuns, CRBI, CWalks,data=train)
#5) [2p] Build regression with GLM on training subset with variables, which have correlation over
# 0,5. Which variables are statistically significant?
glm.model<-glm(Salary ~ CRuns, HmRun, Runs, Walks, Years, CAtBat, CHits, CHmRun,
CRuns, CRBI, CWalks,data=train)
View(Hitters)
#5) [2p] Build regression with GLM on training subset with variables, which have correlation over
# 0,5. Which variables are statistically significant?
glm.model<-glm(Salary ~ CRuns, Hits, HmRun, Runs, Walks, Years, CAtBat, CHits, CHmRun,
CRuns, CRBI, CWalks,data=train, family = binomial(link = "logit"))
#5) [2p] Build regression with GLM on training subset with variables, which have correlation over
# 0,5. Which variables are statistically significant?
glm.model<-glm(Salary ~ CRuns + Hits + HmRun + Runs + Walks + Years + CAtBat +
CHits + CHmRun + CRuns + CRBI + CWalks, data=train, family = binomial(link = "logit"))
View(Hitters)
#5) [2p] Build regression with GLM on training subset with variables, which have correlation over
# 0,5. Which variables are statistically significant?
glm.model<-glm(Salary ~ CRuns + Hits + HmRun + Runs + Walks + Years + CAtBat +
CHits + CHmRun + CRuns + CRBI + CWalks, data=train)
summary(glm.model)
# Intercept and CRuns
# 6) [2p] Build regression with GAM method on training dataset with all variables.
?gam
gam.model<-glm(Salary ~ ., data=train)
summary(gam.model)
#7) [2p] Visualize (bart chart) the Mean Squared Error on training and validation subsets for
#obtained GLM and GAM regression.
glm.pred_train<-as.vector(predict(glm.model,newdata=train))
glm.pred_validate<-as.vector(predict(glm.model,newdata=validate))
gam.pred_train<-as.vector(predict(gam.model,newdata=train))
gam.pred_validate<-as.vector(predict(gam.model,newdata=validate))
mse_glm_train<-mean((train$Salary - glm.pred_train)^2)
mse_gam_train<-mean((train$Salary - gam.pred_train)^2)
mse_glm_valid<-mean((validate$Salary - glm.pred_validate)^2)
mse_gam_valid<-mean((validate$Salary - gam.pred_validate)^2)
wykres <-cbind(mse_glm_train, mse_glm_valid, mse_gam_train, mse_gam_valid)
wykres
barplot(wykres)
# 8) [2p] Based on validation subset check, which model is the best. Show its summary.
summary(glm.pred_validate)
summary(gam.pred_validate)
mse_glm_test<-mean((test$Salary - glm.pred_test)^2)
glm.pred_test<-as.vector(predict(glm.model,newdata=test))
gam.pred_test<-as.vector(predict(gam.model,newdata=test))
#9) [2p] Calculate Mean Squared Error on testing subset for two models. Which model is the best?
mse_glm_test<-mean((test$Salary - glm.pred_test)^2)
mse_gam_test<-mean((test$Salary - gam.pred_test)^2)
# 10) [2p] What the salary would you pay to average player (new exogenous variable is an average
# for every characteristics)? Use the best model, you have obtained.
dane_prognoza<-data.frame(matrix(apply(Hitters[,-ncol(Hitters)] ,2, mean),ncol=16, nrow=1))
colnames(dane_prognoza)<-colnames(Hitters[,-17])
predict(glm.model, newdata = dane_prognoza, type='response', se=F)
# 8. Na podstawie zbioru walidacyjnego sprawdz, kt?ry model jest lepszy, czy regresja GLM, czy regresja GAM.
# Dla najlepszego modelu pokaz tabulogram. Jakie zmienne wchodza do najlepszego modelu?
#porownanie gam vs. glm
library(mgcv)
gam.model <- gam(fmla, data=train)
fmla <- as.formula(paste("Salary ~ .", paste('s(',colnames(train)[-ncol(train)], ')', collapse= "+")))
# 8) [2p] Based on validation subset check, which model is the best. Show its summary.
summary(glm.pred_validate)
summary(gam.pred_validate)
#9) [2p] Calculate Mean Squared Error on testing subset for two models. Which model is the best?
mse_glm_test<-mean((test$Salary - glm.pred_test)^2)
mse_gam_test<-mean((test$Salary - gam.pred_test)^2)
mse_glm_test
mse_gam_test
# 10) [2p] What the salary would you pay to average player (new exogenous variable is an average
# for every characteristics)? Use the best model, you have obtained.
dane_prognoza<-data.frame(matrix(apply(Hitters[,-ncol(Hitters)] ,2, mean),ncol=16, nrow=1))
colnames(dane_prognoza)<-colnames(Hitters[,-17])
predict(glm.model, newdata = dane_prognoza, type='response', se=F)
-ncol(Hitters)
matrix(apply(Hitters[,-ncol(Hitters)] ,2, mean
matrix(apply(Hitters[,-ncol(Hitters)] ,2, mean)
?apply
dane_prognoza
View(dane_prognoza)
?predict
# 10) [2p] What the salary would you pay to average player (new exogenous variable is an average
# for every characteristics)? Use the best model, you have obtained.
prediction<-data.frame(matrix(apply(Hitters[,-ncol(Hitters)] ,2, mean),ncol=16, nrow=1))
colnames(prediction)<-colnames(Hitters[,-17])
predict(glm.model, newdata = prediction, type='response', se=F)
predict(glm.model, newdata = prediction, type='pct', se=F)
predict(glm.model, newdata = prediction, type='terms', se=F)
predict(glm.model, newdata = prediction, type='link', se=F)
predict(glm.model, newdata = prediction, type='repsonse', se=F)
predict(glm.model, newdata = prediction, type='response', se=F)
predict(glm.model, newdata = prediction, type='response')
predict(glm.model, newdata = prediction, type='response')
predict(glm.model, newdata = prediction, type='response', se=T)
predict(glm.model, newdata = prediction, type='response')
# Statistical Learning Methods - Practical test
# clear the workspace, plots and console
rm(list = ls())
if (!is.null(dev.list()))
dev.off()
cat("\014")
# Exercise 1 [20p] - Classification problem
# You are quantitative analyst in medical center. You were asked to help medical staff in decision making process to classify patients with breast cancer into two different therapies. For this purpose, build three concurrent classification models (using Conditional Inference Tree method, Recursive Partitioning and Regression Trees method and Random Forest method) and choose the best one with highest prediction accuracy.
# Before You start:
#   1) Install and load package “mlbench” {install.packages(“mlbench”), library(mlbench)}.
# install.packages("mlbench")
library(mlbench)
# 2) Load dataset named “BreastCancer” {data(BreastCancer)}.
data(BreastCancer)
data <- BreastCancer
typeof(data)
# data <- as.data.frame(data)
# typeof(data)
# 3) Set the seed according to the number of your index.
set.seed(83459)
# 4) Keep informed that the endogenous variable is labeled “Class”.
# 5) Keep informed that if it is possible to show outcome for mini tasks, just copy it into MS Word. In case it is not possible, just copy the instruction to MS Word, which should lead to achieve results. Before you send the file to artur.pluska.sgh@gmail.com, save as PDF.
# To get final results go through mini tasks:
#   1) [2p] How many exogenous variables and observations dataset have? What type of object this data set is?
summary(data)
class(data)
#   2) [2p] Does the dataset contains missing values? If yes, impute missing values with method, that you are free to choose (e.g. average, median, regression of other variables, removing these observations, coding as a separate value, etc.).
any(is.na(data))
data[!complete.cases(data),] # show all columns with incomplete cases
summary(data) # checking which column exactly has NAs
data$Bare.nuclei <- as.numeric(as.character(data$Bare.nuclei))
data$Bare.nuclei[is.na(data$Bare.nuclei)] <- median(data$Bare.nuclei, na.rm = T)
# may be needed (convert all factors to numeric)
# is_num <- c("Cl.thickness","Cell.size","Cell.shape","Marg.adhesion","Epith.c.size",
#           "Bl.cromatin", "Normal.nucleoli", "Mitoses", "Class")
# data[ , is_num] <- lapply(data[, is_num], as.numeric)
# 3) [2p] Remove ID variable and check the percentage of observations for one chosen class.
data$Id <- NULL
length(data$Class[data$Class == "benign"]) / nrow(data)
# 4) [2p] Divide dataset into training subset and validation subset in relation 2:1.
rand <- sample(1:nrow(data), 2/3 * nrow(data))
train <- data[rand,]
valid <- data[-rand,]
# 5) [2p] Build classification tree using Conditional Inference Tree (ctree) method with parameters mincriterion at the level of 0.99 and minsplit equal to 20. Plot obtained model.
# library(partykit)
library(party)
# train <- as.matrix(train)
ctree.model <-
ctree(
Class ~ .,
data = train,
controls = ctree_control(mincriterion = 0.99, minsplit = 20)
)
plot(ctree.model, tnex = 2, type = "extended")
# devAskNewPage(ask = TRUE)
# 6) [2p] Build classification tree using Recursive Partitioning and Regression Trees (rpart) methods with parameters cp at the level of 0.00001 and minsplit equal to 3. Plot chart showing complexity parameter vs. misclassification error. Choose cp with lowest misclassification error.
library(rpart)
rpart.model <- rpart(Class ~ ., train, cp = 0.00001, minsplit = 5)
plotcp(rpart.model)  # Which complexity parmeter gives us the best results
minimum.error <- which.min(rpart.model$cptable[, "xerror"])
minimum.error
optimal.complexity <- rpart.model$cptable[minimum.error, "CP"]
optimal.complexity
points(minimum.error, rpart.model$cptable[minimum.error, "xerror"], col = "red",
pch = 19)
#7) [2p] Prune previous classification tree with optimal cp.
pruned.tree <- prune(rpart.model, cp = optimal.complexity)
plot(pruned.tree, compress = T, uniform = T, margin = 0.1, branch = 0.3, nspace = 2)
text(pruned.tree, use.n = TRUE, pretty = 0)
#8) [2p] Build Random Forest model with number of trees at the level of 250.
library(randomForest)
forest<-randomForest(Class ~ ., data = train, ntree = 250, do.trace = T)
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
plot(forest,col = "black")
varImpPlot(forest, bg = 11)
#9) [2p] For all classification trees make prediction on validation subset. Show confusion matrices
#for every model.
confusion.matrix<-list()
cat("Confusion Matrix - ctree")
print(confusion.matrix[[1]]<-table(predict(ctree.model,new=valid),valid$Class))
ctree_table <- table(predict(ctree.model,new=valid),valid$Class)
cat("\nConfusion Matrix - pruned rpart\n")
print(confusion.matrix[[2]]<-table(predict(pruned.tree,type="class",newdata=valid),valid$Class))
rpart_table <- table(predict(pruned.tree,type="class",newdata=valid),valid$Class)
cat("\nConfusion Matrix - random forest")
print(confusion.matrix[[3]]<-table(predict(forest,newdata= valid),valid$Class))
rf_table <- table(predict(forest,newdata= valid),valid$Class)
cat("\nAccuracy comparison\n")
#10) [2p] Print comparison table which show for every method misclassification error on validation
#subset. Which model is the best?
CalculateAccuracy<-function(confusion.matrix) {return(sum(diag(confusion.matrix))/sum(confusion.matrix))}
print(data.frame(model=c("ctree","pruned rpart","random forest"),accuracy=sapply(confusion.matrix,CalculateAccuracy)),row.names=FALSE)
?apply_vl
library(mercer.reporter)
?apply_vl
!! К
!!?
s
library(ISLR)
setwd("~/Desktop/SGH/2 semester/Statistical learning methods/R")
setwd("~/Desktop/SGH/2 semester/Statistical learning methods/R")
Carseats <- read.csv("Carseats.csv")
library(ROCR)
library(ISLR)
library(rpart)
library(randomForest)
abalone <- read.csv("abalone.csv")
setwd("~/Desktop/SGH/2 semester/Statistical learning methods/R")
abalone <- read.csv("abalone.csv")
abalone <- read.csv2("abalone.csv")
View(abalone)
str(abalone)
# checking missing value
any(is.na(abalone))
# 2. percentage of observations where class equal to 1
length(abalone$Class[abalone$Class == 1]) / nrow(abalone)
# 2. percentage of observations where class equal to 1
length(abalone$Class[abalone$class == 1]) / nrow(abalone)
# 2. percentage of observations where class equal to 1
length(abalone$class[abalone$class == 1]) / nrow(abalone)
# 3. plot with distribution of class variable
barplot(abalone$class)
# 3. plot with distribution of class variable
hist(abalone$class)
dev.off()
# 3. plot with distribution of class variable
hist(abalone$class)
# 4. Mean value of variable ShellWeight where class = 1
abalone$ShellWeight[abalone$class == 1]
library(ROCR)
library(ISLR)
library(rpart)
library(randomForest)
setwd("~/Desktop/SGH/2 semester/Statistical learning methods/R")
# clear the workspace, plots and console
rm(list = ls())
if (!is.null(dev.list()))
dev.off()
cat("\014")
# loading data
abalone <- read.csv2("abalone.csv")
str(abalone)
# 1. checking missing value
any(is.na(abalone)) # there are no NAs
# 2. percentage of observations where class equal to 1
length(abalone$class[abalone$class == 1]) / nrow(abalone) # 0.23%
# 3. plot with distribution of class variable
hist(abalone$class)
# 4. Mean value of variable ShellWeight where class = 1
mean(abalone$ShellWeight[abalone$class == 1])
# 5. Dividing data into training and validation (80 to 20)
rand <- sample(1:nrow(abalone), 0.8 * nrow(abalone))
train <- data[rand,]
valid <- data[-rand,]
train <- abalone[rand,]
valid <- abalone[-rand,]
# 6. decision tree with rpart package with cp = 0.0001
rpart.model <- rpart(class ~ ., train, cp = 0.0001)
# 6. decision tree with rpart package with cp = 0.0001
rpart <- rpart(class ~ ., train, cp = 0.0001)
# 7. checking the model quality with prediction with threshold at 0.2 and creating
#    confusion matrix
pred <- predict(rpart, newdata = valid)
pred <- round(pred, 0)
# 7. checking the model quality with prediction with threshold at 0.2 and creating
#    confusion matrix
pred <- predict(rpart, newdata = valid)
pred <- ifelse(pred > 0.2, 1, 0)
result <- data.frame(pred, valid$target)
result <- data.frame(pred, valid$class)
View(result)
klas <- table(pred, valid$class)
klas
# confusion matrix
tab <- table(pred, valid$class)
acc <- (tab[1,1] + tab[2,2]) / sum(tab) # Accuracy
acc
tpr <- tab[2, 2] / sum(tab[, 2]) # True Positive rate
tpr
tnr <- tab[1, 1] / sum(tab[, 1]) # True Negative
tnr
# How many clients can I reach?
plot(performance(pred,"tpr","rpp"), lwd = 2)
p <- prediction(pred, valid$class)
plot(performance(p,"tpr","rpp"), lwd = 2)
# 8. draw a ROC and Lift curve
p <- prediction(pred, valid$class)
plot(performance(p,"tpr","fpr"), colorize = T)
attr(performance(p, "auc"), "y.values") # AUC - area under the curve
# Lift curve - how much can I gain thanks to model?
# lift = TPR/RPP
plot(performance(p,"lift","rpp"),lwd=2, colorize=T)
# 9. checking AUC and making conclusion
attr(performance(p, "auc"), "y.values") # AUC - area under the curve
# 10. Pruning decision tree from task 6 with cp = 0.005 and making conclusion
pruned.tree <- prune(rpart, cp = 0.005)
summary(pruned.tree)
pred2 <- predict(RPARpruned.treeT.1, newdata = valid)
pred2 <- predict(pruned.tree, newdata = valid)
pred2 <- round(pred2, 0)
result2 <- data.frame(pred2, valid$target)
result2 <- data.frame(pred2, valid$class)
error2 <- sum(abs(pred2 - valid$class)) / nrow(valid)
error2   # 0.1208791
Acc2 <- 1 - error2
Acc2     # 0.8791209
Acc2     # 0.8062201
# Exercise 3
data(ISLR)
library(ISLR)
# Exercise 3
data(ISLR)
# Exercise 3
data('ISLR')
# Exercise 3
data("Carseats")
data <- Carseats
# 1. plot with distribution of sales
hist(data$Sales)
# 2. dividing data on training and validation datasets
set.seed(0)
rand <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[rand,]
valid <- data[-rand,]
# 3. linear regression
linear <- lm(Sales ~ Age + Price + Income  ,data = train)
# 4. again regression but with Age polynomial 6
linear <- lm(Sales ~ poly(Age^6) + Price + Income, data = train)
# 5. random forest with 10 trees
forest <- randomForest(Sales ~ Age + Price + Income, data = train, ntree = 10,
do.trace = T)
# 4. again regression but with Age polynomial 6
linear2 <- lm(Sales ~ poly(Age^6) + Price + Income, data = train)
# 3. linear regression
linear <- lm(Sales ~ Age + Price + Income, data = train)
# 6. comapring quality of the models
confusion.matrix<-list()
cat("Confusion Matrix - ctree")
print(confusion.matrix[[1]]<-table(predict(linear,new=valid),valid$lass))
print(confusion.matrix[[1]]<-table(predict(linear,new=valid),valid$Sales))
linear_table <- table(predict(linear,new=valid),valid$Sales)
cat("\nConfusion Matrix - pruned rpart\n")
linear2
cat("\nConfusion Matrix - linear2")
print(confusion.matrix[[2]]<-table(predict(linear2,type="class",newdata=valid),valid$Sales))
print(confusion.matrix[[2]]<-table(predict(linear2,type="repsonse",newdata=valid),valid$Sales))
print(confusion.matrix[[2]]<-table(predict(linear2,newdata=valid),valid$Sales))
linear2_table <- table(predict(linear2,type="class",newdata=valid),valid$Sales)
linear2_table <- table(predict(linear2, newdata=valid),valid$Sales)
cat("\nConfusion Matrix - random forest")
print(confusion.matrix[[3]]<-table(predict(forest,newdata=valid),valid$Sales))
rf_table <- table(predict(forest,newdata= valid),valid$Sales)
confusion.matrix<-list()
cat("Confusion Matrix - linear")
print(confusion.matrix[[1]]<-table(predict(linear,new=valid),valid$Sales))
linear_table <- table(predict(linear,new=valid),valid$Sales)
cat("\nConfusion Matrix - linear2")
print(confusion.matrix[[2]]<-table(predict(linear2,newdata=valid),valid$Sales))
linear2_table <- table(predict(linear2, newdata=valid),valid$Sales)
cat("\nConfusion Matrix - random forest")
print(confusion.matrix[[3]]<-table(predict(forest,newdata=valid),valid$Sales))
rf_table <- table(predict(forest,newdata= valid),valid$Sales)
cat("\nAccuracy comparison\n")
CalculateAccuracy<-function(confusion.matrix) {return(sum(diag(confusion.matrix))/sum(confusion.matrix))}
print(data.frame(model=c("ctree","pruned rpart","random forest"),accuracy=sapply(confusion.matrix,CalculateAccuracy)),row.names=FALSE)
EvaluateModel <- function(classif_mx){
true_positive <- classif_mx[1,1]
true_negative <- classif_mx[2,2]
condition_positive <- sum(classif_mx[ ,1])
condition_negative <- sum(classif_mx[ ,2])
accuracy <- (true_positive + true_negative) / sum(classif_mx)
sensitivity <- true_positive / condition_positive
specificity <- true_negative / condition_negative
misclassification <- 1 - accuracy
return(list(accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
misclassification = misclassification))
}
EvaluateModel(ctree_table)
EvaluateModel(linear_table)
EvaluateModel(linear2_table)
EvaluateModel(rf_table)
# 6. comapring quality of the models
rf.pred = predict(forest, valid)
confusionMatrix(data = rf.pred, reference = valid$Sales, mode = "everything")
# 6. comapring quality of the models
library(caret)
confusionMatrix(data = rf.pred, reference = valid$Sales, mode = "everything")
rf.pred
valid$Sales
typeof(rf.pred)
typeof(valid$Sales)
rf.pred = predict(forest, valid)
confusionMatrix(data = rf.pred, reference = valid$Sales, mode = "everything")
View(valid)
rf = train(Sales ~ Age + Price + Income, data = train, method = "rf", trControl = train_Control,
metric = "ROC")
train_Control = trainControl(ntree = 10)
EvaluateModel(linear2_table)
mse_linear_test<-mean((test$Salary - glm.pred_test)^2)
linear_pred <-as.vector(predict(linear,newdata=valid))
linear_pred2 <-as.vector(predict(linear2,newdata=valid))
rf_predict <-as.vector(predict(forest,newdata=valid))
mse_linear_pred<-mean((train$Salary - linear_pred)^2)
mse_linear_pred
mse_linear_pred <- mean((train$Salary - linear_pred)^2)
mse_linear2_pred <- mean((train$Salary - linear2_pred)^2)
linear_pred2 <-as.vector(predict(linear2,newdata=valid))
rf_predict <-as.vector(predict(forest,newdata=valid))
mse_linear_pred <- mean((train$Salary - linear_pred)^2)
mse_linear2_pred <- mean((train$Salary - linear2_pred)^2)
linear2_pred <-as.vector(predict(linear2,newdata=valid))
mse_linear2_pred <- mean((train$Salary - linear2_pred)^2)
mse_forest_pred <- mean((train$Salary - forest_pred)^2)
mse_forest_pred <- mean((train$Salary - rf_predict)^2)
mse_forest_pred
mse_linear2_pred <- mean((train$Salary - linear2_pred)^2)
# 3. plot with distribution of class variable
hist(abalone$class)
barplot(abalone$class)
acc
Acc2 > acc
Acc2
acc
gg <- ggplot(data = abalone)
gg + geom_histogram(aes(x = class))
gg + geom_bar(aes(x = class))
gg + geom_bar(aes(x = class, fill = factor(class)))
View(abalone)
gg <- ggplot(data = data)
gg + geom_bar(aes(x = Sales, fill = factor(Sales)))
gg + geom_histogram(aes(x = Sales))
# 1. plot with distribution of sales
hist(data$Sales)
gg + geom_histogram(aes(x = Sales))
gg + geom_histogram(aes(x = Sales, size = 0.5))
gg + geom_histogram(aes(x = Sales, color = "green"))
gg + geom_histogram(aes(x = Sales, fill = "green"))
gg + geom_histogram(aes(x = Sales))
# 2. dividing data on training and validation datasets
set.seed(0)
# 1. plot with distribution of sales
hist(data$Sales)
# Lift curve
plot(performance(p,"lift","rpp"),lwd=2, colorize=T)
