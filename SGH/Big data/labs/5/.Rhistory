use_python(python = Sys.which("python3"), required = TRUE)
py_config()
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
library(keras)
library(keras)
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
py_config()
library(keras)
mnist <- dataset_mnist()
library(reticulate)
py_config()
# -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
py_config()
# -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
mnist <- dataset_mnist()
# -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
py_config()
library(keras)
mnist <- dataset_mnist()
Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2
py_config()
use_python(python = Sys.which("python3"), required = TRUE)
# !!!! -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
reticulate::py_discover_config("keras")
devtools::install_github("rstudio/keras")
reticulate::py_discover_config("keras")
reticulate::py_discover_config("tensorflow")
library(keras)
mnist <- dataset_mnist()
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
install_tensorflow()
install_tensorflow()
devtools::install_github("rstudio/tensorflow")
# Read in MNIST data
mnist <- dataset_mnist()
install_tensorflow()
library(tensorflow)
install_tensorflow()
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
# Read in MNIST data
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
View(mnist)
str(train_images)
str(train_labels)
str(test_images)
str(test_labels)
network <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
View(train_labels)
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
system.time(m <- matrix(rnorm(10000^2), ncol=10000))
m
system.time(m[lower.tri(m)] <- 0)
system.time(sum(m))
View(m)
rm(list=ls())
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
str(train)
dim(train$data)
dim(test$data)
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
variable.summary(GLM)
variable.summary(Wesbrook)
summary(GLM)
Wesbrook$DEPT1 <- as.numeric(Wesbrook$DEPT1)
Wesbrook$FACULTY1 <- as.numeric(Wesbrook$FACULTY1)
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
variable.summary(Wesbrook)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
# Generalized linear model after excluding all insignificant variables
GLM2 <- glm(WESBROOK ~ . , family=binomial(logit), data = Wesbrook.est)
summary(GLM2)
1 - (GLM2$deviance/GLM2$null.deviance) # 0.720267
# Creating Log columns
Wesbrook.est$LogTOTLGIVE <- with(Wesbrook.est, log(TOTLGIVE))
# Model after transforming numerical columns to logs
GLM3<- glm(WESBROOK ~ LogTOTLGIVE + PARENT + CHILD + OTHERACT, family = binomial(logit), data = Wesbrook.est)
summary(GLM3)
1 - (GLM3$deviance/GLM3$null.deviance) # 0.7502665
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
# Loading CCS dataset from BCA package
data(CCS, package="BCA")
# Creating validation and estimation subsets
set.seed(321)
CCS$Sample <- create.samples(CCS, est = 0.7, val = 0.3)
View(CCS)
# Creating numerical variable from factor variable MonthsGive
CCS <- within(CCS, {
MonthGive.Num <- Recode(MonthGive, '"Yes"=1; "No"=0', as.factor=FALSE) # Recode changes values from yes/no to 1/0
})
# Variables distribution chart
scatterplot(MonthGive.Num~AveDonAmt, reg.line=lm, smooth=TRUE,
id.n = 2, boxplots='xy', span=0.5, data=CCS)
# Means chart for Region variable
with(CCS, plotMeans(MonthGive.Num, Region, error.bars="none")) # plotmeans plots mean values of variables
# Reorganizing levels of Region factor variable
CCS$NRegion <- relabel.factor(CCS$Region, new.labels=c('NR1','NR1','NR1',
'NR2','NR2','NR2'))
with(CCS, plotMeans(MonthGive.Num, NRegion))
# Generalized linear model
GLM.2 <- glm(MonthGive ~ Age20t29 +Age20t39 + Age60pls + Age70pls + Age80pls +
AveDonAmt + AveIncEA + DonPerYear + EngPrmLang + FinUnivP + LastDonAmt +
NewDonor+NRegion, family=binomial(logit), data=CCS)
variable.summary(GLM.2)
variable.summary(CCS)
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
variable.summary(Wesbrook)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
View(Wesbrook)
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
})
OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
CHILD <- Recode(CHILD, '"Y"=1; "N"=0', as.factor=FALSE)
SPOUSE <- Recode(SPOUSE, '"Y"=1; "N"=0', as.factor=FALSE)
SEX <- Recode(SEX, '"Y"=1; "N"=0', as.factor=FALSE)
FACSTAFF <- Recode(FACSTAFF, '"Y"=1; "N"=0', as.factor=FALSE)
ATHLTCS <- Recode(ATHLTCS, '"Y"=1; "N"=0', as.factor=FALSE)
OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
# PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
# CHILD <- Recode(CHILD, '"Y"=1; "N"=0', as.factor=FALSE)
# SPOUSE <- Recode(SPOUSE, '"Y"=1; "N"=0', as.factor=FALSE)
# SEX <- Recode(SEX, '"Y"=1; "N"=0', as.factor=FALSE)
# FACSTAFF <- Recode(FACSTAFF, '"Y"=1; "N"=0', as.factor=FALSE)
# ATHLTCS <- Recode(ATHLTCS, '"Y"=1; "N"=0', as.factor=FALSE)
# OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
})
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor = F)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
# Generalized linear model after excluding all insignificant variables
GLM2 <- glm(WESBROOK ~ . , family=binomial(logit), data = Wesbrook.est)
summary(GLM2)
1 - (GLM2$deviance/GLM2$null.deviance) # 0.720267
# Creating Log columns
Wesbrook.est$LogTOTLGIVE <- with(Wesbrook.est, log(TOTLGIVE))
# Model after transforming numerical columns to logs
GLM3<- glm(WESBROOK ~ LogTOTLGIVE + PARENT + CHILD + OTHERACT, family = binomial(logit), data = Wesbrook.est)
summary(GLM3)
1 - (GLM3$deviance/GLM3$null.deviance) # 0.7502665
library(readxl)
library(AMORE) # "A MORE flexible neural network package"
rm(list = ls())
setwd("~/Desktop/SGH/2 semester/Big data/labs/5")
# load data and check its structure
data <- read_excel("data.xls")
str(data)
# checking the number of each group occurences
length(data$`Group of preference`[data$`Group of preference` == 2])   # 51
length(data$`Group of preference`[data$`Group of preference` == 1])   # 49
# dividing data on training and validation datasets
set.seed(231)
rand <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[rand,]
valid <- data[-rand,]
# constructing matrices and vectors for model
WE = as.matrix(train[, 1:4])        # input data as matrix with 4 columns
WY = as.vector(train[, 5])          # output vector
VAL_WE = as.matrix(valid[, 1:4])
VAL_WY = as.vector(valid[, 5])
net <-
newff(
n.neurons = c(4, 10, 10, 1),
learning.rate.global = 1e-2,
momentum.global = 0.5,
error.criterium = "TAO",
Stao = NA,
hidden.layer = "tansig",
output.layer = "purelin",
method = "ADAPTgdwm"           # this is backpropagation method
)
# train function activate NN learning
result <-
train(
net,
WE,
WY,
VAL_WE,
VAL_WY,
error.criterium = "LMS",
report = T,               # Shows steps in console
show.step = 100,          # Number of epochs to train non-stop until the training function is allow to report.
n.shows = 4              # Number of times to report (if report is TRUE).
# The total number of training epochs is n.shows times show.step.
)
# sim function generate ouput from NN
WYSN <- sim(result$net, VAL_WE)
# data frame result compare output from NN with original results
WYSN <- round(WYSN, 0)
result1 <- data.frame(VAL_WY, WYSN)
error <- sum(abs(WYSN - VAL_WY))          # b calculates number of errors
error
# Ploting error function
matplot(
result$Merror,
pch = 21:23,
bg = c("white",  "black"),
type = "o",
col = "black",
xlim = c(1, 30),
ylim = c(0, 0.2)
)
