py_config()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
py_config()
library(keras)
mnist <- dataset_mnist()
Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2
py_config()
use_python(python = Sys.which("python3"), required = TRUE)
# !!!! -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
reticulate::py_discover_config("keras")
devtools::install_github("rstudio/keras")
reticulate::py_discover_config("keras")
reticulate::py_discover_config("tensorflow")
library(keras)
mnist <- dataset_mnist()
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
install_tensorflow()
install_tensorflow()
devtools::install_github("rstudio/tensorflow")
# Read in MNIST data
mnist <- dataset_mnist()
install_tensorflow()
library(tensorflow)
install_tensorflow()
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
# Read in MNIST data
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
View(mnist)
str(train_images)
str(train_labels)
str(test_images)
str(test_labels)
network <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
View(train_labels)
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
system.time(m <- matrix(rnorm(10000^2), ncol=10000))
m
system.time(m[lower.tri(m)] <- 0)
system.time(sum(m))
View(m)
rm(list=ls())
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
str(train)
dim(train$data)
dim(test$data)
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
variable.summary(GLM)
variable.summary(Wesbrook)
summary(GLM)
Wesbrook$DEPT1 <- as.numeric(Wesbrook$DEPT1)
Wesbrook$FACULTY1 <- as.numeric(Wesbrook$FACULTY1)
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
variable.summary(Wesbrook)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
# Generalized linear model after excluding all insignificant variables
GLM2 <- glm(WESBROOK ~ . , family=binomial(logit), data = Wesbrook.est)
summary(GLM2)
1 - (GLM2$deviance/GLM2$null.deviance) # 0.720267
# Creating Log columns
Wesbrook.est$LogTOTLGIVE <- with(Wesbrook.est, log(TOTLGIVE))
# Model after transforming numerical columns to logs
GLM3<- glm(WESBROOK ~ LogTOTLGIVE + PARENT + CHILD + OTHERACT, family = binomial(logit), data = Wesbrook.est)
summary(GLM3)
1 - (GLM3$deviance/GLM3$null.deviance) # 0.7502665
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
# Loading CCS dataset from BCA package
data(CCS, package="BCA")
# Creating validation and estimation subsets
set.seed(321)
CCS$Sample <- create.samples(CCS, est = 0.7, val = 0.3)
View(CCS)
# Creating numerical variable from factor variable MonthsGive
CCS <- within(CCS, {
MonthGive.Num <- Recode(MonthGive, '"Yes"=1; "No"=0', as.factor=FALSE) # Recode changes values from yes/no to 1/0
})
# Variables distribution chart
scatterplot(MonthGive.Num~AveDonAmt, reg.line=lm, smooth=TRUE,
id.n = 2, boxplots='xy', span=0.5, data=CCS)
# Means chart for Region variable
with(CCS, plotMeans(MonthGive.Num, Region, error.bars="none")) # plotmeans plots mean values of variables
# Reorganizing levels of Region factor variable
CCS$NRegion <- relabel.factor(CCS$Region, new.labels=c('NR1','NR1','NR1',
'NR2','NR2','NR2'))
with(CCS, plotMeans(MonthGive.Num, NRegion))
# Generalized linear model
GLM.2 <- glm(MonthGive ~ Age20t29 +Age20t39 + Age60pls + Age70pls + Age80pls +
AveDonAmt + AveIncEA + DonPerYear + EngPrmLang + FinUnivP + LastDonAmt +
NewDonor+NRegion, family=binomial(logit), data=CCS)
variable.summary(GLM.2)
variable.summary(CCS)
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
variable.summary(Wesbrook)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
View(Wesbrook)
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
})
OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
CHILD <- Recode(CHILD, '"Y"=1; "N"=0', as.factor=FALSE)
SPOUSE <- Recode(SPOUSE, '"Y"=1; "N"=0', as.factor=FALSE)
SEX <- Recode(SEX, '"Y"=1; "N"=0', as.factor=FALSE)
FACSTAFF <- Recode(FACSTAFF, '"Y"=1; "N"=0', as.factor=FALSE)
ATHLTCS <- Recode(ATHLTCS, '"Y"=1; "N"=0', as.factor=FALSE)
OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
# PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
# CHILD <- Recode(CHILD, '"Y"=1; "N"=0', as.factor=FALSE)
# SPOUSE <- Recode(SPOUSE, '"Y"=1; "N"=0', as.factor=FALSE)
# SEX <- Recode(SEX, '"Y"=1; "N"=0', as.factor=FALSE)
# FACSTAFF <- Recode(FACSTAFF, '"Y"=1; "N"=0', as.factor=FALSE)
# ATHLTCS <- Recode(ATHLTCS, '"Y"=1; "N"=0', as.factor=FALSE)
# OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
})
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor = F)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
# Generalized linear model after excluding all insignificant variables
GLM2 <- glm(WESBROOK ~ . , family=binomial(logit), data = Wesbrook.est)
summary(GLM2)
1 - (GLM2$deviance/GLM2$null.deviance) # 0.720267
# Creating Log columns
Wesbrook.est$LogTOTLGIVE <- with(Wesbrook.est, log(TOTLGIVE))
# Model after transforming numerical columns to logs
GLM3<- glm(WESBROOK ~ LogTOTLGIVE + PARENT + CHILD + OTHERACT, family = binomial(logit), data = Wesbrook.est)
summary(GLM3)
1 - (GLM3$deviance/GLM3$null.deviance) # 0.7502665
for (i in 1:10) {
print(i)
}
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
View(Wesbrook)
summary(Wesbrook)
summarise(Wesbrook)
?variable.summary
?train
# load the data after check point
setwd("~/Desktop/SGH/2 semester/Big data/Project/Heart disease")
data = read.csv("data.csv")
head(data)
str(data)
# necessary transformation for next models!
data$target = ifelse(data$target == "Yes", 1, 0)
View(data)
str(data)
# load the data after check point
setwd("~/Desktop/SGH/2 semester/Big data/Project/Heart disease")
data = read.csv("data.csv")
head(data)
str(data)
# necessary transformation for next models!
data$target = ifelse(data$target == "Yes", 1, 0)
data$sex = as.factor(data$sex)
# dividing data on training and validation datasets
set.seed(713)
rand <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[rand,]
valid <- data[-rand,]
# ---------------------------------------------------------------------------
# ------------- Bagging -------------
library(ipred)
mod <- bagging(target ~ ., data = train, nbagg = 1000)   # nbagg - number of iterations
a <- predict(mod, newdata = valid)
a <- round(a, 0)
str(train)
# Calculate error and accuracy
wynik <- data.frame(a, valid$target)
error <- sum(abs(a - valid$target))
error        # 14
nrow(valid)  # 91
acc <- (nrow(valid) - error) / nrow(valid)
acc          # 0.8461538
err <- 1 - acc
err          # 0.1538462
# ------------ tree ------------
library(tree)
# building tree
tree_data <- tree(target ~ ., data = train)
summary(tree_data)
# functions plot and text draws graphical representation of the tree
plot(tree_data)
text(tree_data)
tree2_data <-
prune.tree(tree_data, best = 3) # prune.tree determines a nested sequence of
# subtrees of the supplied tree by recursively
# “snipping” off the least important splits.
plot(tree2_data)
text(tree2_data)
# predict outcome in validation data
pred <- predict(tree2_data, newdata = valid)
pred <- round(pred, 0)
result <- data.frame(pred, valid$target)
error <- sum(abs(pred - valid$target)) / nrow(valid)
error      # 0.2197802
accuracy = 1 - error
accuracy
summary(tree2_data)
# ------------- rpart -------------
library(rattle) # package for GUI in R
library(rpart)
RPART.1 <-
rpart(target ~ . , data = train, cp = 0.022) # cp - complexity parameter. larger cp - simplier tree
plotcp(RPART.1)
printcp(RPART.1)  # Pruning Table
RPART.1           # Tree Leaf Summary
# draw plot with text
par(mar = c(1, 1, 1, 1))
plot(RPART.1)
text(RPART.1)
# To obtain more smart diagram the function fancyRpartPlot from rattle package can be used
fancyRpartPlot(RPART.1)
pred2 <- predict(RPART.1, newdata = valid)
pred2 <- round(pred2, 0)
result2 <- data.frame(pred2, valid$target)
error2 <- sum(abs(pred2 - valid$target)) / nrow(valid)
error2   # 0.1208791
Acc2 <- 1 - error2
Acc2     # 0.8791209
# ---------- Neural Network ----------
library(RcmdrPlugin.BCA)
NNET.2 <- Nnet(target ~ ., data = train, decay = 0.00000001, # decay is a
# regularization to avoid over-fitting
size = 12) # size - number of hidden units in the neural network
# ---------- Neural Network ----------
library(RcmdrPlugin.BCA)
NNET.2 <- Nnet(target ~ ., data = train, decay = 0.00000001, # decay is a
# regularization to avoid over-fitting
size = 12) # size - number of hidden units in the neural network
str(data)
NNET.2 <- Nnet(target ~ ., data = train, decay = 0.00000001, # decay is a
# regularization to avoid over-fitting
size = 12) # size - number of hidden units in the neural network
NNET.2$value # Final Objective Function Value
summary(NNET.2)
prediction <- predict(NNET.2, newdata = valid)
prediction <- round(prediction, 0)
result <- data.frame(prediction, valid$target)
error <- sum(abs(prediction - valid$target)) / nrow(valid)
error   # 0.1098901
Acc3 <- 1 - error
Acc3    # 0.9010989
View(data)
