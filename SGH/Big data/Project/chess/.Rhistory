# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
py_config()
# -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
mnist <- dataset_mnist()
# -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
py_config()
library(keras)
mnist <- dataset_mnist()
Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2
py_config()
use_python(python = Sys.which("python3"), required = TRUE)
# !!!! -------------------------------------------------------------------
library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "/Macintosh HD/Applications/Python 3.7.2")
# Sys.setenv(RETICULATE_PYTHON = "/anaconda3/lib/python3.7.2")
Sys.setenv(RETICULATE_PYTHON = "/usr/local/bin/python3")
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
reticulate::py_discover_config("keras")
devtools::install_github("rstudio/keras")
reticulate::py_discover_config("keras")
reticulate::py_discover_config("tensorflow")
library(keras)
mnist <- dataset_mnist()
use_python(python = Sys.which("python3"), required = TRUE)
py_config()
library(keras)
install_tensorflow()
install_tensorflow()
devtools::install_github("rstudio/tensorflow")
# Read in MNIST data
mnist <- dataset_mnist()
install_tensorflow()
library(tensorflow)
install_tensorflow()
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
# Read in MNIST data
mnist <- dataset_mnist()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
View(mnist)
str(train_images)
str(train_labels)
str(test_images)
str(test_labels)
network <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
View(train_labels)
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
system.time(m <- matrix(rnorm(10000^2), ncol=10000))
m
system.time(m[lower.tri(m)] <- 0)
system.time(sum(m))
View(m)
rm(list=ls())
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
str(train)
dim(train$data)
dim(test$data)
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
variable.summary(GLM)
variable.summary(Wesbrook)
summary(GLM)
Wesbrook$DEPT1 <- as.numeric(Wesbrook$DEPT1)
Wesbrook$FACULTY1 <- as.numeric(Wesbrook$FACULTY1)
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
variable.summary(Wesbrook)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
# Generalized linear model after excluding all insignificant variables
GLM2 <- glm(WESBROOK ~ . , family=binomial(logit), data = Wesbrook.est)
summary(GLM2)
1 - (GLM2$deviance/GLM2$null.deviance) # 0.720267
# Creating Log columns
Wesbrook.est$LogTOTLGIVE <- with(Wesbrook.est, log(TOTLGIVE))
# Model after transforming numerical columns to logs
GLM3<- glm(WESBROOK ~ LogTOTLGIVE + PARENT + CHILD + OTHERACT, family = binomial(logit), data = Wesbrook.est)
summary(GLM3)
1 - (GLM3$deviance/GLM3$null.deviance) # 0.7502665
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
# Loading CCS dataset from BCA package
data(CCS, package="BCA")
# Creating validation and estimation subsets
set.seed(321)
CCS$Sample <- create.samples(CCS, est = 0.7, val = 0.3)
View(CCS)
# Creating numerical variable from factor variable MonthsGive
CCS <- within(CCS, {
MonthGive.Num <- Recode(MonthGive, '"Yes"=1; "No"=0', as.factor=FALSE) # Recode changes values from yes/no to 1/0
})
# Variables distribution chart
scatterplot(MonthGive.Num~AveDonAmt, reg.line=lm, smooth=TRUE,
id.n = 2, boxplots='xy', span=0.5, data=CCS)
# Means chart for Region variable
with(CCS, plotMeans(MonthGive.Num, Region, error.bars="none")) # plotmeans plots mean values of variables
# Reorganizing levels of Region factor variable
CCS$NRegion <- relabel.factor(CCS$Region, new.labels=c('NR1','NR1','NR1',
'NR2','NR2','NR2'))
with(CCS, plotMeans(MonthGive.Num, NRegion))
# Generalized linear model
GLM.2 <- glm(MonthGive ~ Age20t29 +Age20t39 + Age60pls + Age70pls + Age80pls +
AveDonAmt + AveIncEA + DonPerYear + EngPrmLang + FinUnivP + LastDonAmt +
NewDonor+NRegion, family=binomial(logit), data=CCS)
variable.summary(GLM.2)
variable.summary(CCS)
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
variable.summary(Wesbrook)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
View(Wesbrook)
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
})
OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
CHILD <- Recode(CHILD, '"Y"=1; "N"=0', as.factor=FALSE)
SPOUSE <- Recode(SPOUSE, '"Y"=1; "N"=0', as.factor=FALSE)
SEX <- Recode(SEX, '"Y"=1; "N"=0', as.factor=FALSE)
FACSTAFF <- Recode(FACSTAFF, '"Y"=1; "N"=0', as.factor=FALSE)
ATHLTCS <- Recode(ATHLTCS, '"Y"=1; "N"=0', as.factor=FALSE)
OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor=FALSE)
# PARENT <- Recode(PARENT, '"Y"=1; "N"=0', as.factor=FALSE)
# CHILD <- Recode(CHILD, '"Y"=1; "N"=0', as.factor=FALSE)
# SPOUSE <- Recode(SPOUSE, '"Y"=1; "N"=0', as.factor=FALSE)
# SEX <- Recode(SEX, '"Y"=1; "N"=0', as.factor=FALSE)
# FACSTAFF <- Recode(FACSTAFF, '"Y"=1; "N"=0', as.factor=FALSE)
# ATHLTCS <- Recode(ATHLTCS, '"Y"=1; "N"=0', as.factor=FALSE)
# OTHERACT <- Recode(OTHERACT, '"Y"=1; "N"=0', as.factor=FALSE)
})
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
# Removing some variables
Wesbrook <- within(Wesbrook, {
ID <- NULL
INDUPDT <- NULL
BIGBLOCK <- NULL # 1 level factor variable
MARITAL <- NULL
MAJOR1 <- NULL
})
# Creating numerical variable from factor variable WESBROOK
Wesbrook <- within(Wesbrook, {
WESBROOK <- Recode(WESBROOK, '"Y"=1; "N"=0', as.factor = F)
})
# Creating estimation and validation subsets
set.seed(323)
Wesbrook$Sample <- create.samples(Wesbrook, est = 0.7, val = 0.3)
# Excluding all NA from Wesbrook
Wesbrook <- na.exclude(Wesbrook)
# Checking if there are still some NA values
any(is.na(Wesbrook))
# Generalized linear model after excluding some variables
GLM <- glm(WESBROOK ~ . , family=binomial(logit), data=Wesbrook)
summary(GLM)
# R2 McFadden value
1 - (GLM$deviance/GLM$null.deviance) # 0.7724513
# Selecting 70% of observations for training model
Wesbrook.est <- Wesbrook %>% filter(Sample == "Estimation")
# Selecting only statistically significant variables and dependent one
Wesbrook.est <- Wesbrook.est[, c("TOTLGIVE", "PARENT", "CHILD", "OTHERACT", "WESBROOK")]
# Generalized linear model after excluding all insignificant variables
GLM2 <- glm(WESBROOK ~ . , family=binomial(logit), data = Wesbrook.est)
summary(GLM2)
1 - (GLM2$deviance/GLM2$null.deviance) # 0.720267
# Creating Log columns
Wesbrook.est$LogTOTLGIVE <- with(Wesbrook.est, log(TOTLGIVE))
# Model after transforming numerical columns to logs
GLM3<- glm(WESBROOK ~ LogTOTLGIVE + PARENT + CHILD + OTHERACT, family = binomial(logit), data = Wesbrook.est)
summary(GLM3)
1 - (GLM3$deviance/GLM3$null.deviance) # 0.7502665
for (i in 1:10) {
print(i)
}
library(BCA)
library(relimp)
library(car)
library(RcmdrMisc)
library(foreign)
library(corrplot)
library(dplyr)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
rm(list=ls())                      # clean environment
if(!is.null(dev.list())) dev.off() # clean all plots
cat("\014")                        # clean console
# Loading data
Wesbrook <- read.dbf("~/Desktop/SGH/2 semester/Big data/labs/4/Wesbrook.dbf", as.is=FALSE)
variable.summary(Wesbrook)
View(Wesbrook)
summary(Wesbrook)
summarise(Wesbrook)
?variable.summary
getwd()
library(rchess)
library(bigchess)
library(dplyr)
library(ggplot2)
library(readr)
library(plyr)
library(dplyr)
library(nnet)
library(MASS)
library(rpart)
library(caret)
library(randomForest)
library(reshape2)
library(rpart.plot)
library(rattle)
library(gbm)
# clear the workspace, plots and console
rm(list = ls())
if (!is.null(dev.list()))
dev.off()
cat("\014")
setwd("~/Desktop/SGH/2 semester/Big data/Project/chess")
# load the data after check point
chess = read.csv("chess.csv")
head(chess)
# --------------------- Data Exploration ---------------------
library(ggplot2)
ggplot(data = chess, aes(x = adjusted_elo_difference, y = winner)) + geom_point()
# white wins percentage
length(which(chess$winner == "white")) / length(chess$winner) # 0.3061709
# draw percentage
length(which(chess$winner == "draw")) / length(chess$winner)  # 0.4992089
# black wins percentage
length(which(chess$winner == "black")) / length(chess$winner) # 0.1946203
# Now check out the distribution of adjusted_elo_differences and make sure
# it isnâ€™t skewed so that white tends to be better. Looks normally distributed
# around 0.
hist(chess$elo_difference)
# does it really make sense???))
gg <- ggplot(chess)
gg + geom_histogram(data = chess, aes(elo_difference, fill = winner))
# dividing data on training and validation datasets
set.seed(713)
rand <- sample(1:nrow(chess), 0.7 * nrow(chess))
train <- chess[rand, ]
valid <- chess[-rand, ]
# ------------ GBM ------------       it works!!!
# 5 folds cross validiation
fitControl <-
trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
# generate GBM model
fit <-
train(
winner ~ adjusted_elo_difference + eco_cat + adjusted_white_elo +
adjusted_black_elo,
data = train,
method = "gbm",
trControl = fitControl,
verbose = FALSE
)
# accuracy for training data
confusionMatrix(predict(fit, newdata = train), train$winner) # 55.4%
# use model to predict for test data
predict <- predict(fit, newdata = valid)
# find accuracy of test data
confusionMatrix(predict, valid$winner) # 55.3%
